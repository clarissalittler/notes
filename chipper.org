#+STARTUP: latexpreview
* Computer Architecture Notes
** Clock signals
   https://en.wikipedia.org/wiki/Clock_signal
** App C: Pipelining
    I find the wiki entry more readable than the chapter, but we'll go ahead and use both:  https://en.wikipedia.org/wiki/Instruction_pipelining 
https://en.wikipedia.org/wiki/Branch_predictor
   
    What's the basic point of pipelining? Well, ideally it's that instruction execution is split into separate phases such that each clock cycle we can start on a new instruction and move all the others currently "in the pipeline" further down the line. The phases given for the example architecture in the book splits things into five phases for each instruction:
    1. instruction fetching
    2. instruction decode and register fetching
    3. alu instruction execution
    4. loading/writing memory
    5. writing back to result registers

 So in principle you should have an average of 1 instruction finishing per clock cycle, but reality rarely mimics the ideal. The first example given in the text for pipeline fail was if there was only a single communication channel with memory, so after a load instruction is started then when the load actually needs to touch the memory the start of the next instruction, which involves fetching the instruction from memory first, needs to be delayed by a cycle.

Branching is obviously also going to be a problem because if you incorrectly predict the branching then you're going to start processing instructions that you weren't supposed to execute. Now, one of the ways to deal with this is just to proceed as if the branch wasn't taken and, if indeed it wasn't, just continue onwards but if it was then you can cancel the instruction you're currently working on and then in the next cycle load the instruction at the updated PC. You can, apparently, also try reordering things so that you perform instructions that are guaranteed to occur whether or not the branch is taken while you're waiting to compute the branch address. The text calls this delayed branching. As for branch prediction, there's a couple of kinds of branch prediction that the text mentions. The first is what they call "static prediction" which involves ill-described schema for deciding whether a branch should be assumed taken or assumed not-taken. The logic behind this is that most branches are the same every time. Of course, you can't predict all of these things in advance perfectly without running afoul of the halting problem, but I can totally imagine that you can have pretty good heuristics and that's really what all of this is about: heuristics. Dynamic branching involving having a number of bits to keep track of the branching history in order to modify the predictions. If you have just one bit then you're going to predict the last result of the branch (or whatever other branch mapped to it in the tiny collision prone hash table) and if it's wrong then we update the prediction. If you have n-bits then you only change the prediction on the nth mistake. 

Ah, also, there's possible dependency of ALU calculations on further instructions. For example, if you have something like an add that puts a value in a register that's then used in a multiply instruction in the next instruction, you're not going to do the writeback the first add into the register until long after you've retrieved the out of date values for the mult instruction. How do you prevent this? Well, basically you need to have a temporary storage for the results of the various phases so that you can "forward" the values. I'm not entirely sure how this is implemented, but the principle is simple enough. You have some way to keep track of whether or not you're referring to a value that's stored as a temporary result and then getting it from the location. I imagine that this slightly slows down each clock cycle, but it is probably worth it for the pipelining benefit. 
*** Implementing Pipelining
    In the first section, we deal with how the 5-clock cycle phase of an unpipelined simplified MIPS.

    1. Instruction fetch cycle
       1. IR <- Mem[PC]
       2. NPC <- PC + 4; (i.e. temporary program counter incremented by four bytes)
    2. Instruction decode/ register fetch
       1. A <- Regs[rs]
       2. B <- Regs[rt]
       3. Imm <- sign-extended immediate field of IR
    3. different phases depending on the instruction
       - memory reference :: output <- A + Imm (indexing into memory)
       - register only calculations :: output <- op A B
       - register-immediate calculation :: output <- op A Imm
       - branch calculation :: output <- NPC + (Imm << 2); cond <- (A == 0)
    4. memory access/branch completion
       - all phases update the PC first from NPC
       - memory load :: lmd <- mem[output]
       - memory write :: mem[output] <- B
       - branch :: if cond then pc <- output
    5. write output into registers

Now to turn this into a pipelined version we need to add registers between each phase. These registers hold the data from our intermmediary registers in the above version.

**** Addenda
     I don't find a lot of the details about how to connect everything particularly interesting. From skimming, as far as I can tell there's a lot of details about how to forward the information between registers to deal with data dependency in pipelining and details about how and where to deal with the pipelining hazards. For example, the PC is going to be written whenever an IF phase ends and we have the ID cycle compute the branch target and the cond-test. This means adding another arithmetic module just for the ID phase since the normal alu module is busy. Whenver 
**** Difficulty of implementation
     Exceptions are the main thing discussed here. Since synchronous instructions happen within the instruction themselves. When an exception occurs then we need to save the state of the instructions and get back to them later. If there wasn't a branch we just go back to the saved instruction and continue fetching, otherwise we need to evaluate the branch condition and go from there. Delayed branches complicate this because we simultaneously need to know the branch that we're supposed to be evaluating as well as the pointers to the instructions we're supposed to be executing while we compute the branch.

     Skimming this section it sounds like the MIPS example is one of the easier architectures to handle exceptions. In comparison there are some architectures where operations can take more than one clock cycle, which is a definite complication, and architectures where state is changing in the middle of an instruction being execute rather than at the well-defined operation points of MIPS.

     The tl;dr lesson of this section and the previous one is that a certain amount of overhead is needed to make sure that pipelining has as few delays as possible and that the particular of the instruction set design makes a big difference about how easy it is to deal with interrupts.
*** Random Bits on Forwarding
    Alright, so because this was something I figured out while studying but never wrote down I want to do so now:
    there's not just data forwarding because of loads or depending on the result of the alu, but rather there's also data forwarding when it comes to *jumps*. Since you don't want to only calculate when you need the jump after having loaded several other instructions into the pipeline, instead we can make calculating if the jump will be taken as a part of the instruction decoding phase. This means, though, that we need more forwarding to the ID phase to make this reasonable without introducing yet another source of stalls
** Ch 1
*** Estimating reliability
    Well the main thing we covered on the topic is that mean time to failure of any one of an independent set of components can be estimated as the inverse of the sum of their failures-per-unit-time. For a set of redundant components, we estimate the reliability to be 

\[
   \frac{\frac{MTTF_{ind}}{n}}{\frac{MTTR^{n-1}_{ind}}{MTTF^n_{ind}}}
\]

which reduces to 

\[
  \frac{MTTF^n_{ind}}{n MTTR^{n-1}}
\]

or in the case of just two components this is simply 

\[
  \frac{MTTF^{2}}{2 MTTR}
\]

Oh, so because I was silly and messed this up before I want to make a note that if there *is* no time to repair the short hand for estimating the MTTF for a collection of devices involves taking the number of failures per unit time and then dividing the number of failures needed for a collective failure by the failures per unit time. 

I overthought this massively and tried to use the fact the memorylessness to come up with what I thought was a more accurate equation, but I messed it up. This is all based in approximations on [[https://en.wikipedia.org/wiki/Poisson_distribution][Poisson distributions]] which is what I should read a bit more about if I want to feel more comfortable with the approximations. I'm a terrible overthinker when I don't know the full story.
*** Power usage
    Dynamic power of switching a transistor is ~~ capacitive load x V^2, which means that whenever we can reduce the voltage we're going to save on energy proportional to the square in the reduction.
*** Benchmarks
    Basically the point of this section is that most benchmarks are useless because they've been over-engineered so as to be completely not objective measures of performance. They're either rigged, too trivial, or don't generally reflect real usage. The book does, however, like the SPEC set of benchmarks and talks about how performance should be compared with the geometric mean of ratios of the performance of a computer on the program by the reference computer performance, i.e. 

\[
\frac{Execution_{reference}}{Execution_{test}}
\]

as the rations, and as a reminder because this is a concept I use about once a lifetime, here's the wiki page for the [[https://en.wikipedia.org/wiki/Geometric_mean][geometric mean]].
*** Amdahl's Law
\[
  speedup_{overall} = \frac{1}{(1-\text{frac}) + \frac{frac}{speedup_{enhanced}}}
\]
** Ch 3: More on Pipelining
   https://en.wikipedia.org/wiki/Superscalar_processor
*** Instruction Level Parallelism
**** Dependencies
***** Data Dependencies
      The more obviously named kind of dependency, where data being *computed* by one instruction 
      is being used by other instructions, so something like

      #+BEGIN_EXAMPLE
      LOOP: L.D F0 0(R1)
            ADD.D F4,F0,F2
            S.D F4,0(R1)
            DADDUI R1,R1,#-8
            BNE R1,R2,LOOP
      #+END_EXAMPLE
      then instruction 2 has a data dependence on instruction 1
      and instruction 3 has a data dependence on instruction 2
      and similarly instruction 5 is dependent on instruction 4
***** Name Dependencies
      Name dependence means that something like
      #+BEGIN_EXAMPLE
      ADD.D F4,F0,F1
      SUB.D F4,F8,F9
      #+END_EXAMPLE
      which means that you can't swap the order of these since, in the case of an interrupt or something like that we *need* to keep the order right

      the other thing is something like
      #+BEGIN_EXAMPLE
      ADD.D F0,F1,F2
      SUB.D F1,F2,F3
      #+END_EXAMPLE

      which means that you can't swap the order of events without causing the wrong data from the register to be fetched
***** Control Dependence
      this is a little more subtle and is about the ways that instructions being executed depends on the control flow. Basically, we need to be certain that exceptions and data flow are preserved properly. Any time that an instruction is executed when its not supposed to it can mess either of these things up.
***** Dependency chart 
      | Instruction producing result | Instruction using result | Latency |
      | FP ALU                       | FP ALU                   |       3 |
      | FP ALU                       | S.D                      |       2 |
      | L.D                          | FP ALU                   |       1 |
      | L.D                          | S.D                      |       0 |
***** Branch prediction
      The most simple form of branch prediction is just keeping a simple set of bits to determine what to do based on prior branches. For example, if you have a single bit then if the bit is a 0 you're going to assume that the branch is not-taken, but if the bit is 1 you're going to assume that the branch is taken. If you're wrong, you flip the bit. That's great, right? The reason why this works so well is that, in general, when you're doing something like a loop you're going to repeat the action you just did almost every time. 

      Now, you can always bump it up to multiple bits and do something like the following scheme: the msb is going to determine our prediction, and if we're wrong we start appropriately flipping the less significant bits until we end up flipping the msb again. So something like
      00 -> branch taken -> 01 -> branch taken -> 11 -> branch not taken -> 1 0 -> branch taken -> 1 1
      something like that. 

      NOTE: with this schema it applies to all branches globally. There's only one set of bits for this prediction. Different branches are all going to mess around with these bits.

      There's also correlated branch prediction, which is pretty cute: you keep around n*2^m bits so you can make a prediction based on the previous m branches using n bits to make the decision based on prior accuracy, so its a 2-fold notion of history.

      Tournament selection: alternate between local and global prediction based on past performance. (Don't really understand much about this idea yet)
***** Loop Unrolling
      Loop unrolling, in this context, is a way of giving yourself a lot more to play with in terms of having instructions to shuffle around for reordering. That's at least the very rough motivation. So let's take a simple example such as
      #+BEGIN_EXAMPLE
      LOOP : 
      L.D F0, 0(R1)
      ADD.D F4, F0, F2
      S.D F4, 0(R1)
      DADDUI R1,R1, #-8
      BNE R1, R2, LOOP
      #+END_EXAMPLE
then because of necessary stalling this is actually going to be
      #+BEGIN_EXAMPLE
      LOOP : 
      L.D F0, 0(R1)
      -
      ADD.D F4, F0, F2
      -
      -
      S.D F4, 0(R1)
      DADDUI R1,R1, #-8
      -
      BNE R1, R2, LOOP
      #+END_EXAMPLE
      and unrolled two times it's going to look something more like
      #+BEGIN_EXAMPLE
      LOOP:
      L.D F0, 0(R1)
      L.D F1, -8(R1)
      ADD.D F4, F0, F2
      ADD.D F5, F1, F2
      DADDUI R1,R1, #-16
      S.D F4, 16(R1)
      S.D F5, 8(R1)
      BNE R1, R2, LOOP
      #+END_EXAMPLE

Now how about some other examples? Let's say something like 
      #+BEGIN_EXAMPLE
      LOOP:
      L.D F0,0(R1)
      L.D F2,0(R2)
      -
      MULT.D F4,F0,F2
      -
      -
      -
      ADD.D F4,F4,F6
      -
      -
      S.D F4,0(R1)
      DADDUI R1,R1,#-8
      DADDUI R2,R2,#-8
      BNE R1,R3, LOOP
      #+END_EXAMPLE
then our unrolling three times is going to look like
     #+BEGIN_EXAMPLE
      LOOP: 
      L.D F0,0(R1)
      L.3 F2,0(R2)
      -
      MULT.D F4,F0,F2
      -
      -
      -
      ADD.D F4,F4,F6
      -
      -
      S.D F4,0(R1)
      L.D F8,-8(R1)
      L.D F10,-8(R2)
      -
      MULT.D F12,F8,F10
      -
      -
      -
      ADD.D F12,F12,F6
      -
      -
      S.D F12,-8(R1)      
      L.D F14,-16(R1)
      L.D F16,-16(R2)
      -
      MULT.D F18,F14,F16
      -
      -
      -
      ADD.D F18,F18,F6
      -
      -
      S.D F18,-16(R1)
      DADDUI R1,R1,#-24
      DADDUI R2,R2,#-24
      BNE R1,R3, LOOP      
     #+END_EXAMPLE
     Now to schedule this it's going to look like
     #+BEGIN_EXAMPLE
      LOOP: 
      L.D F0,0(R1)
      L.D F2,0(R2)
      L.D F8,-8(R1)
      L.D F10,-8(R2)
      L.D F14,-16(R1)
      L.D F16,-16(R2)
      MULT.D F4,F0,F2
      MULT.D F12,F8,F10
      MULT.D F18,F14,F16
      DADDUI R1,R1,#-24
      DADDUI R2,R2,#-24
      ADD.D F4,F4,F6
      ADD.D F12,F12,F6
      ADD.D F18,F18,F6
      S.D F4,24(R1)
      S.D F12,16(R1)      
      S.D F18,8(R1)
      BNE R1,R3, LOOP      
     #+END_EXAMPLE
***** Scoreboarding
 
